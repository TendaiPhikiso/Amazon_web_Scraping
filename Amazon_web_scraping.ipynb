{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72378192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Fetch the maximum page number for the search\n",
    "def get_maxPageNumber(HEADERS):\n",
    "    # Webpage URL\n",
    "    URL = \"https://www.amazon.co.uk/s?k=data+books&crid=3TFWSUVPVID3R&sprefix=data+books%2Caps%2C86&ref=nb_sb_noss_1\"\n",
    "    \n",
    "    # HTTP Request\n",
    "    try:\n",
    "        webpage = requests.get(URL, headers=HEADERS)\n",
    "        webpage.raise_for_status()  # Raise HTTPError for bad responses (4xx and 5xx)\n",
    "    except RequestException as e:\n",
    "        print(f\"Failed to retrieve the main page: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    # Soup object containing all data extracted from Amazon\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")  # Converting to HTML format from bytes\n",
    "    \n",
    "    # Total number of pages\n",
    "    pagenumber = soup.find('span',attrs={'class':'s-pagination-item s-pagination-disabled'}).text\n",
    "    return int(pagenumber.strip())\n",
    "\n",
    "def get_links(URL, HEADERS):\n",
    "    try:\n",
    "        webpage = requests.get(URL, headers=HEADERS)\n",
    "        webpage.raise_for_status()  # Raise HTTPError for bad responses (4xx and 5xx)\n",
    "    except RequestException as e:\n",
    "        print(f\"Failed to retrieve the main page: {e}\")\n",
    "        return []  # Return an empty list if there's an error\n",
    "        \n",
    "    # Soup object containing all data extracted from Amazon\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")  # Converting to HTML format from bytes\n",
    "       \n",
    "    # Fetching links as List of Tag objects \n",
    "    links = soup.find_all(\"a\", attrs={'class': \"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"})\n",
    "    \n",
    "    # Link list \n",
    "    links_list = []\n",
    "    \n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and href.startswith(\"/\"):\n",
    "            href = \"https://www.amazon.co.uk\" + href\n",
    "        links_list.append(href)\n",
    "    \n",
    "    return links_list  # Make sure to return the list of links\n",
    "\n",
    "\n",
    "# Function to extract book title\n",
    "def get_bookTitle(soup):\n",
    "    try:\n",
    "        title = soup.find(\"span\", attrs={\"id\": 'productTitle'})\n",
    "        title_value = title.text\n",
    "        title_string = title_value.strip()\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "    return title_string\n",
    "\n",
    "# Function to extract author name\n",
    "def get_authorName(soup):\n",
    "    try:\n",
    "        name = soup.find('span', attrs={'class': 'author notFaded'}).text\n",
    "        name_string = name.strip()\n",
    "    except AttributeError:\n",
    "        name_string = \"\"\n",
    "    return name_string\n",
    "\n",
    "# Function to extract book price\n",
    "def get_sellingPrice(soup):\n",
    "    try:\n",
    "        sellingPrice = soup.find('span', attrs={'class': 'a-price aok-align-center reinventPricePriceToPayMargin priceToPay'}).text.strip()\n",
    "    except AttributeError:\n",
    "        sellingPrice = \"\"\n",
    "    return sellingPrice\n",
    "\n",
    "# Function to get book listing price\n",
    "def get_listingPrice(soup):\n",
    "    try:\n",
    "        listingPrice = soup.find('span', attrs={'class': 'a-size-small aok-offscreen'}).text.strip()\n",
    "    except AttributeError:\n",
    "        listingPrice = \"\"\n",
    "    return listingPrice\n",
    "\n",
    "# Function to get book type\n",
    "def get_bookType(soup):\n",
    "    try:\n",
    "        typeOfBook = soup.find(\"span\", attrs={\"id\": 'productSubtitle'}).text.strip()\n",
    "    except AttributeError:\n",
    "        typeOfBook = \"\"\n",
    "    return typeOfBook\n",
    "\n",
    "# Function to get book page length\n",
    "def get_printLength(soup):\n",
    "    try:\n",
    "        printLength = soup.find('div', attrs={'id': 'rpi-attribute-book_details-fiona_pages'}).text.strip()\n",
    "    except AttributeError:\n",
    "        printLength = \"\"\n",
    "    return printLength\n",
    "\n",
    "def get_publicationDate(soup):\n",
    "    try:\n",
    "        publicationDate = soup.find('div', attrs={'id': 'rpi-attribute-book_details-publication_date'}).text.strip()\n",
    "    except AttributeError:\n",
    "        publicationDate = \"\"\n",
    "    return publicationDate\n",
    "\n",
    "# Function to extract book ratings\n",
    "def get_bookRating(soup):\n",
    "    try:\n",
    "        rating = soup.find(\"span\", attrs={\"class\": 'a-icon-alt'}).text.strip()\n",
    "    except AttributeError:\n",
    "        rating = \"\"\n",
    "    return rating\n",
    "\n",
    "# Function to extract review count\n",
    "def get_reviewCount(soup):\n",
    "    try:\n",
    "        reviews = soup.find('span', attrs={'id': 'acrCustomerReviewText'}).text.strip()\n",
    "    except AttributeError:\n",
    "        reviews = \"\"\n",
    "    return reviews\n",
    "\n",
    "# Function to extract book availability - In stock or out of stock\n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        available = soup.find('div', attrs={'id': 'availability'}).text.strip()\n",
    "    except AttributeError:\n",
    "        available = \"Out of stock\"\n",
    "    return available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe711254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    HEADERS = { 'User-Agent':'',\n",
    "               'Accept-Language': 'en-GB,en;q=0.9'}\n",
    "    \n",
    "    max_pageNumber =  2  # Set to a fixed number for testing\n",
    "    # max_pageNumber =  get_maxPageNumber(HEADERS)\n",
    "    \n",
    "    all_links = []\n",
    "    all_links.append(get_links('https://www.amazon.co.uk/s?k=data+books&crid=3TFWSUVPVID3R&sprefix=data+books%2Caps%2C86&ref=nb_sb_noss_1', HEADERS))\n",
    "    \n",
    "    # Loop through each page starting from page 2\n",
    "    for i in range(2, max_pageNumber + 1):\n",
    "        URL = f'https://www.amazon.co.uk/s?k=data+books&page={i}&crid=3TFWSUVPVID3R&qid=1721330789&sprefix=data+books%2Caps%2C86&ref=sr_pg_{i}'\n",
    "        all_links.append(get_links(URL, HEADERS))\n",
    "        \n",
    "    all_links = [item for sublist in all_links for item in sublist]  # Flatten the list of lists\n",
    "    \n",
    "    d = {\"title\": [], \"name\": [], \"sellingPrice\": [],\"listingPrice\": [],\"typeOfBook\": [],\"printLength\": [], \"publicationDate\": [],\"rating\": [], \"reviews\": [],\"availability\": []}\n",
    "    \n",
    "    # Loop to extract book details from each link  \n",
    "    for link in all_links:\n",
    "        try:\n",
    "            new_webpage = requests.get(link, headers=HEADERS)\n",
    "            new_webpage.raise_for_status()  # Raise HTTPError for bad responses\n",
    "            new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "            \n",
    "            # Function calls to display all book details\n",
    "            d['title'].append(get_bookTitle(new_soup))\n",
    "            d['name'].append(get_authorName(new_soup))\n",
    "            d['sellingPrice'].append(get_sellingPrice(new_soup))\n",
    "            d['listingPrice'].append(get_listingPrice(new_soup))\n",
    "            d['typeOfBook'].append(get_bookType(new_soup))\n",
    "            d['printLength'].append(get_printLength(new_soup))\n",
    "            d['publicationDate'].append(get_publicationDate(new_soup))\n",
    "            d['rating'].append(get_bookRating(new_soup))\n",
    "            d['reviews'].append(get_reviewCount(new_soup))\n",
    "            d['availability'].append(get_availability(new_soup))\n",
    "            \n",
    "            # Delay between requests\n",
    "            time.sleep(2)  # Sleep for 2 seconds\n",
    "        except RequestException as e:\n",
    "            print(f\"Failed to retrieve the page: {e}\")\n",
    "            continue\n",
    "        \n",
    "    amazon_df = pd.DataFrame.from_dict(d)\n",
    "    #amazon_df.to_csv(\"amazon_data.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
